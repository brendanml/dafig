{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'_id': 'sw0007', 'name': 'Anakin Skywalker (Brown Aviator Cap)', 'image': 'img.bricklink.com/ItemImage/MN/0/sw0007.png'}\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from requests_html import AsyncHTMLSession\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# URL to scrape\n",
    "url = \"https://www.bricklink.com/v2/catalog/catalogitem.page?M=sw0007#T=S&O={%22rpp%22:%22500%22,%22iconly%22:0}\"\n",
    "\n",
    "# Headers for the request\n",
    "headers = {\n",
    "    \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36\",\n",
    "    \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8\",\n",
    "    \"Accept-Language\": \"en-US,en;q=0.5\",\n",
    "    \"Accept-Encoding\": \"gzip, deflate, br\",\n",
    "    \"Connection\": \"keep-alive\",\n",
    "    \"Upgrade-Insecure-Requests\": \"1\",\n",
    "}\n",
    "\n",
    "# Define an async function for scraping\n",
    "async def fetch_and_render():\n",
    "    session = AsyncHTMLSession()\n",
    "    res = await session.get(url, headers=headers)  # Use 'await' to retrieve the result\n",
    "    await res.html.arender()  # Render JavaScript\n",
    "    return res.html\n",
    "\n",
    "# Run the async function\n",
    "asession = AsyncHTMLSession()\n",
    "\n",
    "html_content = await fetch_and_render()  # Fetch the rendered HTML\n",
    "\n",
    "# Use BeautifulSoup to parse the HTML\n",
    "soup = BeautifulSoup(html_content.html, \"html.parser\")\n",
    "\n",
    "# print(soup)\n",
    "\n",
    "items = soup.find_all(\"tr\", class_=\"pciItemContents\")\n",
    "\n",
    "listings = []\n",
    "stores = {}\n",
    "figure_name = soup.find(\"h1\", id=\"item-name-title\").get_text(strip=True)\n",
    "figure_id = soup.find(\"span\", style=\"font-weight: bold; color: #2C6EA5\").get_text(strip=True)\n",
    "fig_image_url = soup.find(\"img\", class_=\"pciImageMain\")[\"src\"].split(\"//\")[1]\n",
    "\n",
    "figure = {\"_id\":figure_id, \"name\":figure_name, \"image\": fig_image_url}\n",
    "\n",
    "\n",
    "# <td style=\"text-align: center;\" width=\"110px\">Used <b>(2)</b><span class=\"js-item-status-incomplete\" style=\"font-size: 10px; color: #666666; display: none;\"><br/>(Incomplete)</span><span class=\"js-item-status-sealed\" style=\"font-size: 10px; color: #0000cc; display: none;\"><br/>(Sealed)</span></td>\n",
    "\n",
    "# Loop through each item and extract the necessary details\n",
    "store_in_url_pattern = r'(?<=\\.com/)[^?]+'\n",
    "\n",
    "for item in items:\n",
    "\n",
    "    # Extract the store name\n",
    "    store_name = item.find(\"span\", class_=\"pspStoreName\").get_text(strip=True)\n",
    "    \n",
    "    # Extract the store link\n",
    "    url = item.find(\"a\", class_=\"pciItemNameLink\")[\"href\"].split(\"//\")[1]\n",
    "    store_link = url.split(\"?\")[0]\n",
    "    listing_link = url\n",
    "    listing_id = url.split('itemID=')[1]\n",
    "    \n",
    "    # Extract the price\n",
    "    price = item.find(\"td\", style=\"text-align: right;\").get_text(strip=True).split(\"\\n\")[0]\n",
    "    price = price.split(\"(\")[0].strip()\n",
    "    \n",
    "    # Extract the country\n",
    "    country = item.find(\"span\", style=\"font-size: 11px;\").get_text(strip=True).split(\"\\n\")[0]\n",
    "    country = country.split(\"Min\")[0].strip()\n",
    "\n",
    "    condition_quant = item.find(\"td\", {\"style\": \"text-align: center;\", \"width\": \"110px\"}).get_text(strip=True).split(\"(\")\n",
    "    condition = condition_quant[0]\n",
    "    quantity = condition_quant[1][0]\n",
    "\n",
    "    store_id = re.search(store_in_url_pattern, store_link).group(0)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    stores[store_id]= {\n",
    "        \"_id\": store_id,\n",
    "        \"name\": store_name,\n",
    "        \"url\": store_link,\n",
    "        \"country\": country,\n",
    "    }\n",
    "\n",
    "    \n",
    "    # Add the listing to the dictionary\n",
    "    listings.append({\n",
    "        \"_id\": listing_id,\n",
    "        \"url\": listing_link,\n",
    "        \"price\": price,\n",
    "        \"condition\": condition,\n",
    "        \"quantity\": quantity,\n",
    "        \"store_id\": store_id\n",
    "    })\n",
    "\n",
    "ids_to_keep = [listing[\"_id\"] for listing in listings]\n",
    "print(figure)\n",
    "# Print the listings\n",
    "# for listing in listings:\n",
    "#     print(listing)\n",
    "\n",
    "# for key, val in stores.items():\n",
    "#     print(val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/brendanlynch/code/dafig/fig_scraper\n",
      "/Users/brendanlynch/code/dafig/backend/.env\n",
      "mongodb+srv://directbrendan:PwsFvmSkUR1uqkMk@cluster-me.m69jryw.mongodb.net/dafig?retryWrites=true&w=majority\n"
     ]
    }
   ],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "\n",
    "print(BASE_DIR)\n",
    "env_path = BASE_DIR.parent / 'backend' / '.env'\n",
    "print(env_path)\n",
    "\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "\n",
    "print(MONGODB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wrote to db for listing collections\n",
      "wrote to db for store collection\n"
     ]
    }
   ],
   "source": [
    "client = MongoClient(MONGODB_URI)\n",
    "\n",
    "db = client[\"dafig\"]\n",
    "listing_collection = db[\"listings\"]\n",
    "store_collection = db[\"stores\"]\n",
    "\n",
    "listing_operations = [\n",
    "    UpdateOne({\"_id\": listing[\"_id\"]}, {\"$set\": listing}, upsert=True) for listing in listings\n",
    "]\n",
    "\n",
    "if listing_operations:\n",
    "    listing_collection.bulk_write(listing_operations)\n",
    "    print(\"wrote to db for listing collections\")\n",
    "\n",
    "listing_collection.delete_many({\"_id\": {\"$nin\": ids_to_keep}})\n",
    "\n",
    "store_operations = [\n",
    "    UpdateOne({\"_id\": store[\"_id\"]}, {\"$set\": store}, upsert=True) for key, store in stores.items()\n",
    "]\n",
    "\n",
    "if store_operations:\n",
    "    store_collection.bulk_write(store_operations)\n",
    "    print(\"wrote to db for store collection\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify(), 'html')\n",
    "inventory_links = soup.find_all(\"a\", href=lambda href: href and \"catalogItemInv.asp\" in href)\n",
    "\n",
    "# Extract and print the inventory links\n",
    "for link in inventory_links:\n",
    "    href = link['href']\n",
    "    text = link.get_text(strip=True)  # Extract the visible text\n",
    "    print(f\"Link: {href} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find('tr', class_=\"pciItemContents\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure ChromeDriver\n",
    "service = Service('/usr/local/bin/chromedriver')\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode\n",
    "options.add_argument(\"--disable-gpu\")  # Disable GPU for faster performance\n",
    "options.add_argument(\"--blink-settings=imagesEnabled=false\")  # Disable images\n",
    "\n",
    "# Create the WebDriver\n",
    "driver = Chrome(service=service, options=options)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.bricklink.com/v2/catalog/catalogitem.page?M=sw0956#T=S&O={%22rpp%22:%22500%22,%22iconly%22:0}')\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"pciItemContents\"))\n",
    ")\n",
    "# Wait for the elements to load\n",
    "try:\n",
    "    # Wait up to 15 seconds for elements with the class \"pciItemContents\" to appear\n",
    "    elements = WebDriverWait(driver, 120).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, \"pciItemContents\"))\n",
    "    )\n",
    "    \n",
    "    # Iterate through the elements and print their text content\n",
    "    for idx, element in enumerate(elements, start=1):\n",
    "        print(f\"Element {idx}: {element}\")\n",
    "        print(element.text)\n",
    "\n",
    "    print(len(elements))  # Print the number of elements found\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
