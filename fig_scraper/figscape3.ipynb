{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from requests_html import AsyncHTMLSession\n",
    "from pymongo import MongoClient, UpdateOne\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import urllib.parse\n",
    "import re\n",
    "with open(\"swFigs.json\", \"r\") as fig_file:\n",
    "    fig_data = json.load(fig_file)\n",
    "\n",
    "\n",
    "# print(fig_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1463\n",
      "3\n",
      "Fetching sw0002..., Skipping malformed URL: [%strBuyLinkUrl%]\n",
      "No data found for: sw0002\n",
      "Fetching sw0003..., Skipping malformed URL: [%strBuyLinkUrl%]\n",
      "No data found for: sw0003\n",
      "Fetching sw0004..., Skipping malformed URL: [%strBuyLinkUrl%]\n",
      "No data found for: sw0004\n",
      "8\n",
      "Fetching sw0005..., "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import json\n",
    "import urllib.parse\n",
    "from requests_html import AsyncHTMLSession\n",
    "from bs4 import BeautifulSoup\n",
    "from fake_useragent import UserAgent\n",
    "import time\n",
    "import random\n",
    "import math\n",
    "import asyncio\n",
    "\n",
    "ua = UserAgent()\n",
    "\n",
    "\n",
    "async def fetch_and_render(session, url, headers, retries=3, timeout=30):\n",
    "    \"\"\"Fetch and render a page using an existing session, with optional retries.\"\"\"\n",
    "    await asyncio.sleep(random.uniform(1, 3))\n",
    "\n",
    "    for attempt in range(1, retries + 1):\n",
    "\n",
    "        try:\n",
    "            res = await session.get(url, headers=headers)\n",
    "            await res.html.arender(timeout=timeout)\n",
    "            return res.html\n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt}/{retries} failed for {url}: {e}\")\n",
    "            if attempt < retries:\n",
    "                await asyncio.sleep(2)\n",
    "    return None\n",
    "\n",
    "\n",
    "def soupify(html):\n",
    "    soup = BeautifulSoup(html.html, \"html.parser\")\n",
    "\n",
    "    figure_name = soup.find(\"h1\", id=\"item-name-title\")\n",
    "    if not figure_name:\n",
    "        print(\"No figure name found!\")\n",
    "        return None, {}, {}\n",
    "\n",
    "    figure_name = figure_name.get_text(strip=True)\n",
    "\n",
    "    figure_id_tag = soup.find(\"span\", style=\"font-weight: bold; color: #2C6EA5\")\n",
    "    if not figure_id_tag:\n",
    "        print(\"No figure ID found!\")\n",
    "        return None, {}, {}\n",
    "\n",
    "    figure_id = figure_id_tag.get_text(strip=True)\n",
    "\n",
    "    fig_image = soup.find(\"img\", class_=\"pciImageMain\")\n",
    "    fig_image_url = fig_image[\"src\"].split(\"//\")[1] if fig_image and \"src\" in fig_image.attrs else None\n",
    "\n",
    "    if not fig_image_url:\n",
    "        print(\"No image URL found!\")\n",
    "        return None, {}, {}\n",
    "\n",
    "    items = soup.find_all(\"tr\", class_=\"pciItemContents\")\n",
    "    listings = {}\n",
    "    stores = {}\n",
    "\n",
    "    store_in_url_pattern = r'(?<=\\.com/)[^?]+'\n",
    "\n",
    "    for item in items:\n",
    "        anchor = item.find(\"a\", class_=\"pciItemNameLink\")\n",
    "        if not anchor or not anchor.has_attr(\"href\"):\n",
    "            print(\"Skipping: No anchor or missing href\")\n",
    "            continue \n",
    "\n",
    "        href_val = anchor[\"href\"]\n",
    "        parts = href_val.split(\"//\")\n",
    "        if len(parts) < 2 or \"itemID=\" not in parts[1]:\n",
    "            print(f\"Skipping malformed URL: {href_val}\")\n",
    "            continue\n",
    "\n",
    "        url_part = parts[1]\n",
    "\n",
    "        store_name_tag = item.find(\"span\", class_=\"pspStoreName\")\n",
    "        store_name = store_name_tag.get_text(strip=True) if store_name_tag else \"Unknown Store\"\n",
    "\n",
    "        store_link = url_part.split(\"?\")[0]\n",
    "        listing_link = url_part\n",
    "        listing_id = url_part.split(\"itemID=\")[1]\n",
    "\n",
    "        price_cell = item.find(\"td\", style=\"text-align: right;\")\n",
    "        price = price_cell.get_text(strip=True).split(\"(\")[0].strip() if price_cell else \"Unknown Price\"\n",
    "\n",
    "        country_cell = item.find(\"span\", style=\"font-size: 11px;\")\n",
    "        country = country_cell.get_text(strip=True).split(\"Min\")[0].strip() if country_cell else \"Unknown Country\"\n",
    "\n",
    "        cond_cell = item.find(\"td\", {\"style\": \"text-align: center;\", \"width\": \"110px\"})\n",
    "        condition_quant = cond_cell.get_text(strip=True).split(\"(\") if cond_cell else [\"Unknown\", \"1\"]\n",
    "        condition = condition_quant[0].strip()\n",
    "        quantity = condition_quant[1][0] if len(condition_quant) > 1 else \"1\"\n",
    "\n",
    "        store_id_match = re.search(store_in_url_pattern, store_link)\n",
    "        store_id = store_id_match.group(0) if store_id_match else \"unknown_store_id\"\n",
    "\n",
    "        stores[store_id] = {\n",
    "            \"_id\": store_id,\n",
    "            \"name\": store_name,\n",
    "            \"url\": store_link,\n",
    "            \"country\": country,\n",
    "        }\n",
    "\n",
    "        listings[listing_id] = {\n",
    "            \"_id\": listing_id,\n",
    "            \"fig_id\": figure_id,\n",
    "            \"url\": listing_link,\n",
    "            \"price\": price,\n",
    "            \"condition\": condition,\n",
    "            \"quantity\": quantity,\n",
    "            \"store_id\": store_id\n",
    "        }\n",
    "\n",
    "    minifig = {\n",
    "        \"_id\": figure_id,\n",
    "        \"name\": figure_name,\n",
    "        \"image\": fig_image_url,\n",
    "        \"listings\": list(listings.keys())\n",
    "    }\n",
    "\n",
    "    return minifig, listings, stores\n",
    "\n",
    "\n",
    "def fetch_bricklink_data(figs, session, request_count):\n",
    "    \"\"\"Fetch data for a list of figs using the existing session.\"\"\"\n",
    "    params = {\"rpp\": \"500\", \"iconly\": 0}\n",
    "    encoded_params = urllib.parse.quote(json.dumps(params))\n",
    "\n",
    "    headers = {\n",
    "        \"User-Agent\": ua.random,\n",
    "        \"Accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,\"\n",
    "                  \"image/avif,image/webp,*/*;q=0.8\",\n",
    "    }\n",
    "\n",
    "    accumulate_minifigs = []\n",
    "    accumulate_listings = []\n",
    "    accumulate_stores = []\n",
    "\n",
    "    for i in range(math.ceil(request_count)):\n",
    "        try:\n",
    "            print(f\"Fetching {figs[i]['ITEMID']}...\", end=\", \")\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to fetch {figs[i]['ITEMID']}: {e}\")\n",
    "            break\n",
    "        url = f\"https://www.bricklink.com/v2/catalog/catalogitem.page?M={figs[i]['ITEMID']}#T=S&O={encoded_params}\"\n",
    "        soup_success = False\n",
    "        while(not soup_success):\n",
    "            soup_success = True # DISABLE IT FOR NOW\n",
    "            # Build a zero-arg callable that returns our coroutine\n",
    "            # This is necessary for session.run(...) to schedule it\n",
    "            def fetch_job():\n",
    "                return fetch_and_render(session, url, headers, retries=3, timeout=30)\n",
    "\n",
    "            # `session.run` returns a list of results. We only have one job, so index 0\n",
    "            rendered_html = session.run(fetch_job)[0]\n",
    "            if rendered_html is None:\n",
    "                # Retry failed, skip\n",
    "                print(f\"Failed to render {url} after retries.\")\n",
    "                continue\n",
    "\n",
    "\n",
    "\n",
    "            # Once we have the rendered_html, parse it with soupify\n",
    "            try:\n",
    "                minifig, listings, stores = soupify(rendered_html)\n",
    "                if(minifig and listings and stores):\n",
    "                    accumulate_minifigs.append(minifig)\n",
    "                    accumulate_listings.append(listings)\n",
    "                    accumulate_stores.append(stores)\n",
    "                else: \n",
    "                    print(f\"No data found for: {figs[i]['ITEMID']}\")\n",
    "                soup_success = True\n",
    "            except Exception as e:\n",
    "                print(f\"Failed to parse {url}: {e}\")\n",
    "\n",
    "    return accumulate_minifigs, accumulate_listings, accumulate_stores\n",
    "\n",
    "    # rendered_html = session.run(fetch_and_render)[0] # one subroutine, index the first response to it\n",
    "    \n",
    "\n",
    "# --- Usage in Jupyter ---\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Now you can safely call this function:\n",
    "all_listings = []\n",
    "all_stores = []\n",
    "all_minifigs = []\n",
    "# minifig, listings, stores = fetch_bricklink_data(\"sw0007\")\n",
    "\n",
    "# print(minifig)\n",
    "# print(listings)\n",
    "# all_listings.append(listings)\n",
    "# all_stores.append(stores)\n",
    "# all_minifigs.append(minifig)\n",
    "\n",
    "\n",
    "# minifig, listings, stores = fetch_bricklink_data(fig_data[0][\"ITEMID\"])\n",
    "\n",
    "\n",
    "# i = random.uniform(1)\n",
    "print(len(fig_data))\n",
    "\n",
    "i = 0\n",
    "while(i < len(fig_data)):\n",
    "    # time.sleep(random.uniform(1, 3))\n",
    "    session = AsyncHTMLSession()\n",
    "    request_count = math.ceil(random.uniform(2, 6))\n",
    "    stop_index = min(i+request_count, len(fig_data))\n",
    "    print(stop_index)\n",
    "    accumulate_minifig, accumulate_listings, accumulate_stores = fetch_bricklink_data(fig_data[i:stop_index:1], session, request_count)\n",
    "    if(accumulate_minifig and accumulate_listings and accumulate_stores):\n",
    "        for fig in accumulate_minifig:\n",
    "            all_minifigs.append(fig)\n",
    "        for store in accumulate_stores: \n",
    "            all_stores.append(store)\n",
    "        for listing in accumulate_listings:\n",
    "            all_listings.append(listing)\n",
    "    i+=request_count\n",
    "    await session.close()\n",
    "    # else:\n",
    "    #     print(f\"No data found for: {fig['ITEMID']}\")\n",
    "    \n",
    "print(all_minifigs)\n",
    "print(all_stores)\n",
    "print(all_listings)\n",
    "\n",
    "# for fig in all_minifigs:\n",
    "#     print(fig)\n",
    "# for stores in all_stores:\n",
    "#     print(stores)\n",
    "# for listings in all_listings:\n",
    "#     print(listings)\n",
    "\n",
    "# print(minifig)\n",
    "# print(listings)\n",
    "# all_listings.append(listings)\n",
    "# all_stores.append(stores)\n",
    "# all_minifigs.append(minifig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR = Path.cwd()\n",
    "\n",
    "\n",
    "print(BASE_DIR)\n",
    "env_path = BASE_DIR.parent / 'backend' / '.env'\n",
    "print(env_path)\n",
    "\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "MONGODB_URI = os.getenv(\"MONGODB_URI\")\n",
    "\n",
    "print(MONGODB_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient, UpdateOne\n",
    "\n",
    "# Connect to MongoDB\n",
    "client = MongoClient(MONGODB_URI)\n",
    "db = client[\"dafig\"]\n",
    "\n",
    "# Collection mappings\n",
    "collections = {\n",
    "    \"listings\": db[\"listings\"],\n",
    "    \"stores\": db[\"stores\"],\n",
    "    \"minifigs\": db[\"minifigs\"]\n",
    "}\n",
    "\n",
    "# Clear collections (if needed)\n",
    "collections[\"listings\"].delete_many({})\n",
    "collections[\"stores\"].delete_many({})\n",
    "\n",
    "# only delete listings for figs that have successfully been scraped\n",
    "fig_ids = [fig[\"_id\"] for fig in all_minifigs]\n",
    "collections[\"listings\"].delete_many({\"fig_id\": {\"$in\": fig_ids}})\n",
    "\n",
    "\n",
    "# Data mappings\n",
    "data_sets = {\n",
    "    \"listings\": all_listings,\n",
    "    \"stores\": all_stores,\n",
    "    \"minifigs\": all_minifigs\n",
    "}\n",
    "\n",
    "# Function to perform bulk upserts\n",
    "def bulk_upsert(collection, data, is_dict=True):\n",
    "    operations = [\n",
    "        UpdateOne({\"_id\": item[\"_id\"]}, {\"$set\": item}, upsert=True) \n",
    "        for dataset in data for key, item in (dataset.items() if is_dict else [(None, dataset)])\n",
    "    ]\n",
    "    \n",
    "    if operations:\n",
    "        collection.bulk_write(operations)\n",
    "        print(f\"Wrote {len(operations)} documents to {collection.name}\")\n",
    "\n",
    "# Execute bulk upserts\n",
    "bulk_upsert(collections[\"listings\"], data_sets[\"listings\"], is_dict=True)\n",
    "bulk_upsert(collections[\"stores\"], data_sets[\"stores\"], is_dict=True)\n",
    "bulk_upsert(collections[\"minifigs\"], data_sets[\"minifigs\"], is_dict=False)  # Not a dict, just a list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "listing_collection.delete_many({})\n",
    "store_collection.delete_many({})\n",
    "minifig_collection.delete_many({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(soup.prettify(), 'html')\n",
    "inventory_links = soup.find_all(\"a\", href=lambda href: href and \"catalogItemInv.asp\" in href)\n",
    "\n",
    "# Extract and print the inventory links\n",
    "for link in inventory_links:\n",
    "    href = link['href']\n",
    "    text = link.get_text(strip=True)  # Extract the visible text\n",
    "    print(f\"Link: {href} | Text: {text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = soup.find('tr', class_=\"pciItemContents\")\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Configure ChromeDriver\n",
    "service = Service('/usr/local/bin/chromedriver')\n",
    "\n",
    "options = Options()\n",
    "options.add_argument(\"--headless\")  # Run in headless mode\n",
    "options.add_argument(\"--disable-gpu\")  # Disable GPU for faster performance\n",
    "options.add_argument(\"--blink-settings=imagesEnabled=false\")  # Disable images\n",
    "\n",
    "# Create the WebDriver\n",
    "driver = Chrome(service=service, options=options)\n",
    "\n",
    "# Open the webpage\n",
    "driver.get('https://www.bricklink.com/v2/catalog/catalogitem.page?M=sw0956#T=S&O={%22rpp%22:%22500%22,%22iconly%22:0}')\n",
    "WebDriverWait(driver, 60).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"pciItemContents\"))\n",
    ")\n",
    "# Wait for the elements to load\n",
    "try:\n",
    "    # Wait up to 15 seconds for elements with the class \"pciItemContents\" to appear\n",
    "    elements = WebDriverWait(driver, 120).until(\n",
    "        EC.presence_of_all_elements_located((By.CLASS_NAME, \"pciItemContents\"))\n",
    "    )\n",
    "    \n",
    "    # Iterate through the elements and print their text content\n",
    "    for idx, element in enumerate(elements, start=1):\n",
    "        print(f\"Element {idx}: {element}\")\n",
    "        print(element.text)\n",
    "\n",
    "    print(len(elements))  # Print the number of elements found\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "\n",
    "finally:\n",
    "    # Close the WebDriver\n",
    "    driver.quit()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
